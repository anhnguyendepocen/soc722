---
title: "Probability, Part I"
author: "<small><br>Stephen Vaisey<br>Duke University</small>"
date: "<span style = 'font-size: 65%;'>Last update: `r Sys.Date()`<br><br>`r icon::fa_link()` <a href='stephenvaisey.com'><font color='F3F2F1'>stephenvaisey.com</font></a><br>`r icon::fa_twitter()` <a href='http://twitter.com/vaiseys'><font color='F3F2F1'>@vaiseys</font></a><br>`r icon::fa_github()` <a href='https://github.com/vaiseys'><font color='F3F2F1'>vaiseys</font></a></span> "
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, addons.css]
    self_contained: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["left", "middle", "inverse"]
---

```{r setup, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  fig.asp = 0.618,
  fig.retina = 3,
  fig.width = 6,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  out.width = "80%"
)
options(knitr.table.format = "html")
options(knitr.kable.NA = '   ')

library(here)
library(knitr)
library(tidyverse)
library(broom)
library(DiagrammeR)
theme_set(theme_minimal(base_family = "Lato"))

```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#00539B" ,
  white_color = "#F3F2F1" ,
  black_color = "#262626" ,
  header_h1_font_size = "110px",
  header_h2_font_size = "60px",
  header_h3_font_size = "45px",
  text_font_size = "28px" ,
  code_font_size = ".65em" ,
  header_color = "#C84E00" ,
  text_slide_number_font_size = "0.6em" ,
  code_highlight_color = "rgba(161,183,13,0.5)" ,
  link_color = "#993399" ,
  text_font_google = google_font("Lato", "400", "400i", "700") ,
  code_font_google = google_font("Roboto Mono", "400") ,
  extra_css = list(
    ".remark-slide-content" = list("padding-top" = "60px") ,
    "h1" = list("margin" = "0 0 20px 0") ,
    "h2" = list("margin" = "0 0 20px 0") ,
    "h3" = list("margin" = "0 0 20px 0") ,
    "th, td" = list( "padding" = "10px" ) )
)

```

class: center, middle

# Ask two questions

---
class: center, middle

# Assessment time

---

## Today's agenda

1. The big picture

2. Probability

---
class: center,middle,inverse

# The big picture 
## Why are we learning statistics?


---

## Hypotheses, processes, models

.pull-left[
```{r, out.width = "100%"}

grViz("
graph process_mods {

  # a 'graph' statement
  graph [layout = dot,
         rankdir = LR]

  # nodes
  node [fontsize = 5.5, fixedsize = TRUE, fontname = Lato]
  
  node [shape = egg]
  
    h1 [label = 'H1: \n class origins matter']
    h0 [label = 'H0: \n class origins irrelevant']

  node [shape = rectangle]

    p1 [label = 'P1: direct transfers']
    p2 [label = 'P2: cultural capital' ]
    p3 [label = 'P3: heritability']
    p4 [label = 'P4: nominal \n meritocracy']
    
  node [shape = circle]
    m1 [label = 'M1: \n r(PC) > 0'] 
    m2 [label = 'M2: \n r(PC) = 0']

  edge[color = black]
    h0 -- {p3 p4}
    h1 -- {p1 p2}
    {p1 p2 p3} -- m1
    p4 -- m2
    
  subgraph P {
    rank = same; p1; p2; p3; p4;
  }

}", height = 500 )


```
]

.pull-right[

Hypotheses and statistics are connected by **process models** that posit how
the social world actually works. 

There isn't usually a 1:1 mapping between models and hypotheses but different
processes leave different "traces" that can be detected.

]

???

By "origins" you mean the effect of growing up in a certain class.

---
class: center,inverse,middle

# Probability

---

## Two main interpretations

### Frequentist

- *interpretation*: long-run frequency
- *typical examples*: repeatable events like dice or coins

--

### Bayesian

- *interpretation*: subjective probability
- *typical examples*: predictions like rain and elections
  
The *main* difference for right now is in interpretation rather than 
computation.

---

## Probability concepts

1. **Experiment:** an action or set of actions that produce random events of 
interest

--

2. **Sample space:** the set of all possible outcomes of the experiment, 
typically denoted by $\Omega$.

--

3. **Event:** a subset of the sample space

---

## Probability axioms

1. The probability of any event is non-negative:
$P(A) \ge 0$

--

2. The probability that one of the outcomes in the sample space occurs is 1:
$P(\Omega) = 1$.

--

3. (*Addition rule*) If the events are mutually exclusive, then
$P(A \text{ or } B) = P(A) + P(B)$

---

.pull-left[

![](img/seatmap.jpg)

]

.pull-right[

Compute the following:

$P(window)$

$P(aisle)$

$P(window \text{ or } aisle)$

$P(exit\text{ }row)$
]

---
class: inverse
background-image: url(img/dice.jpg)
background-size: cover

--

.footnote[How many possibilities are there?]

---

## The "Garden of Forking Data"

.footnote[The following discussion is adapted from McElreath's *Statistical Rethinking*]

### The future

- many possible paths
- each choice forecloses others

--

### The data

- many possible events
- each observation eliminates some

---

## Example 1: marbles

- Imagine you have a bag of three marbles. Either two of them are blue and one
of them is white (H1) or one of them is blue and two of them are white (H2).

--

- Before you try reach into the bag at all, what is your best guess about 
whether H1 or H2 is more likely?

--

- Now you pull out one marble and it's blue. What is the probability that H1
is true?

--

- You replace the first marble and try again. It's also blue. What is the 
probability now that H1 is true?

---

## Plausibility and counting the ways

.pull-left[
```{r}
grViz("
graph twoblue {

  # a 'graph' statement
  graph [layout = neato, rankdir = LR]

  node [shape = plaintext, label = 'H1 \n DGP', fontname = Lato]
  start

  # nodes
  node [fillcolor = DodgerBlue, penwidth = .4, shape = circle, label = ' ', 
  style = filled]
  
  h1 h2 
  
  h1h1 h1h2 h2h1 h2h2 t3h1 t3h2
  
  node [fillcolor = white]
    
  t3 h1t3 h2t3 t3t3
  
  edge [penwidth = .4]
  
  start -- {h1 h2 t3}
  
  h1 -- {h1h1 h1h2 h1t3}
  
  h2 -- {h2h1 h2h2 h2t3}
  
  t3 -- {t3h1 t3h2 t3t3}
  
  }", height = 400 )

```
]

.pull-right[
```{r}
grViz("
graph oneblue {

  # a 'graph' statement
  graph [layout = neato, rankdir = LR]

  node [shape = plaintext, label = 'H2 \n DGP', fontname = Lato]
  start

  # nodes
  node [fillcolor = DodgerBlue, penwidth = .4, shape = circle, label = ' ', 
  style = filled]
  
  h1 
  
  h1h1  t2h1  t3h1 
  
  node [fillcolor = white]
    
  t3 h1t3 t2t3 t3t3 t2 h1t2 t2t2 t3t2
  
  edge [penwidth = .4]
  
  start -- {h1 t2 t3}
  
  h1 -- {h1h1 h1t2 h1t3}
  
  t2 -- {t2h1 t2t2 t2t3}
  
  t3 -- {t3h1 t3t2 t3t3}


}", height = 400 )

```

]

--

.small[There are two ways to get a blue if H1 is true and only 1 way if it's
false, thus H1 is *twice as plausible*. There are four ways to get two blues if 
H1 is true and one way if it's false, thus H1 is *four times as plausible*.]
--

 .small[ So the probability that H1 is true is 2/3 at the first step ("two to 
 one") and 4/5 at the second ("four to one").]

---

## Bayes' Rule

.pull-left[
![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif)
]

.pull-right[

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$

]

---

## A classic example

.pull-left[
- city of 100,000 people
- 100 vampires
- vampire test
  - positive test 99% of the time for vampires
  - positive test 1% of the time for non-vampires
- what is the probability that someone with a positive test is a vampire?
]

.pull-right[

$$P(V|T) = \frac{P(T|V)P(V)}{P(T)} =$$

$$\frac{.99 \times .001}{.001\times.99 + .999 \times .01} \approx .09$$

]

---

## Odds representation of Bayes' Rule

- **Probability** of $A$ is $\frac{A}{A+\neg A}$
- **Odds** of $A$ is $\frac{A}{\neg A}$
- **Likelihood** ratio is a formalized version of plausibility: the probability 
of getting the data when the hypothesis is *true* divided by the probability 
when it's *false*
--

- Posterior odds = prior odds $\times$ likelihood ratio

$$\frac{99}{999} = \frac{1}{999} \times \frac{99}{1}$$
--

- So the posterior probability given a positive test is $\frac{99}{99+999} \approx .09$

---

## Another example from the book

- 1 in 378 babies born to a 35-year-old woman will have Down syndrome (DS)
- 86% chance of positive ultrasound test if DS
- 5% chance of positive ultrasound test if no DS
- What is the probability of DS given a positive test?
--

$$\frac{1}{377} \times \frac{86}{5} = \frac{86}{1885} \approx .04$$
---

## More on likelihood

- Probability of seeing a particular test result conditional on a value of the parameter
  - likelihood of flipping heads given that *p* = .5 (*L* = .5)
  - likelihood of flipping heads given that *p* = .9 (*L* = .9)
--

- If we were trying to tell two coins apart (.blue[*p* = .25] vs. 
.red[*p* = .75]) what's the likelihood ratio of a single flip?
- If we began with a flat prior, what's the posterior probability that it's the red 
coin if we get heads on that one flip?
--

- What if we believed beforehand that $\frac{3}{4}$ of coins were blue?


???

LR of a single flip is 3

FLAT PRIOR
Posterior ODDS are 3/1; Posterior PROB is 3/4

OTHER PRIOR
Prior odds red = 1/3
LR = 3/1
Posterior odds = 3/3
Posterior prob = 50%

---

## Example 2: private numbers

- In the last example, we knew that there were only two possibilities (H1 or 
H2). What if the answer is more open ended?

--

- Let's say you were curious about what proportion of the time people screen
calls from "private numbers." The answer must be somewhere between 0 and 1, but
let's say you have no idea at all what the proportion is.

--

- Let's say you got a list of valid phone numbers and started calling randomly.
After 9 calls, here are your results: {Screen, Answer, Screen, Screen, Screen, Answer, Screen, Answer, Screen}. 

- What inferences could you make at this point?

---

## Updating with data

```{r}

## copied code from Solomon Kurz
## copied context from McElreath (just changed to something more social)

d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w"))


d <-
  d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w"))


sequence_length <- 50

d %>% 
  expand(nesting(n_trials, toss, n_success), 
         p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
  mutate(lagged_n_trials  = lag(n_trials,  k = 1),
         lagged_n_success = lag(n_success, k = 1)) %>% 
  ungroup() %>% 
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)
  ) %>% 
  group_by(n_trials) %>% 
  mutate(prior      = prior      / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>%   
  
  # plot!
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior), linetype = 2) +
  geom_line(aes(y = likelihood)) +
  scale_x_continuous("proportion screened", breaks = c(0, .5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme_gray() +
  theme(panel.grid = element_blank()) +
  facet_wrap(~strip, scales = "free_y")


```

---

## What more data gives us

- New data not only moves the "most likely" point around, it also gives us 
more confidence about where the true value lies. 

--

- On the last slide, trial 6 and trial 9 both had the same maximum likelihood
value (.667), but the 9th trial's function is thinner (more certain).

---

class: middle, center

# Questions?
