---
title: "Uncertainty, Part II"
author: "<small><br>Stephen Vaisey<br>Duke University</small>"
date: "<span style = 'font-size: 65%;'>Last update: `r Sys.Date()`<br><br>`r icon::fa_link()` <a href='stephenvaisey.com'><font color='F3F2F1'>stephenvaisey.com</font></a><br>`r icon::fa_twitter()` <a href='http://twitter.com/vaiseys'><font color='F3F2F1'>@vaiseys</font></a><br>`r icon::fa_github()` <a href='https://github.com/vaiseys'><font color='F3F2F1'>vaiseys</font></a></span> "
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    self_contained: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["left", "middle", "inverse"]
---

```{r setup, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  fig.asp = 0.618,
  fig.retina = 3,
  fig.width = 6,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  out.width = "80%"
)
options(knitr.table.format = "html")
options(knitr.kable.NA = '   ')

library(here)
library(knitr)
library(tidyverse)
library(broom)
library(DiagrammeR)
theme_set(theme_minimal(base_family = "Lato"))

```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#00539B" ,
  white_color = "#F3F2F1" ,
  black_color = "#262626" ,
  header_h1_font_size = "110px",
  header_h2_font_size = "60px",
  header_h3_font_size = "45px",
  text_font_size = "28px" ,
  code_font_size = ".65em" ,
  header_color = "#C84E00" ,
  text_slide_number_font_size = "0.6em" ,
  code_highlight_color = "rgba(161,183,13,0.5)" ,
  link_color = "#993399" ,
  text_font_google = google_font("Lato", "400", "400i", "700") ,
  code_font_google = google_font("Roboto Mono", "400") ,
  extra_css = list(
    ".remark-slide-content" = list("padding-top" = "60px") ,
    "h1" = list("margin" = "0 0 20px 0") ,
    "h2" = list("margin" = "0 0 20px 0") ,
    "h3" = list("margin" = "0 0 20px 0") ,
    "th, td" = list( "padding" = "10px" ),
    ".small" = list("font-size" = "60%"),
    ".small-code" = list("font-size" = ".6em") ,
    ".smaller-code" = list("font-size" = ".5em") ,
    ".red" = list("color" = "red") ,
    ".blue" = list("color" = "blue") ,
    ".col-text" = list("font-size" = "80%"))
)


```

class: center, middle

# Ask two questions

---

class: center, middle

# Assessment time

---

## Agenda

1. Confidence intervals (continued)

2. Hypothesis testing

---

class:inverse,middle,center

# Confidence intervals

---

## Quantifying uncertainty

- Standard errors are the main ingredient in helping us quantify the 
uncertainty in our estimates.

- Confidence intervals are one of the main things we "cook" with these 
ingredients.

- Confidence intervals allow us to summarize the sampling distribution by
giving plausible upper and lower bounds of our estimate.

---

## CLT and the normal distribution

```{r normal_crits}

score = 1.96
m = 0
std = 1

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + 
  stat_function(fun = dnorm, args = list(mean = m, sd = std)) + 
  stat_function(fun = dnorm, args = list(mean = m, sd = std), xlim = c(score, 4),
                geom = "area", fill = "red", alpha = .2) +
  stat_function(fun = dnorm, args = list(mean = m, sd = std), xlim = c(-4, -score),
                geom = "area", fill = "red", alpha = .2) +
  scale_x_continuous(name = "z-value", breaks = seq(-4, 4, std)) +
  labs(title = "95% Confidence Interval", 
       subtitle = "(+/- 1.96 SD)")

```

---

## CLT and the normal distribution

```{r normal_crit2}

score = 2.58
m = 0
std = 1

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + 
  stat_function(fun = dnorm, args = list(mean = m, sd = std)) + 
  stat_function(fun = dnorm, args = list(mean = m, sd = std), xlim = c(score, 4),
                geom = "area", fill = "red", alpha = .2) +
  stat_function(fun = dnorm, args = list(mean = m, sd = std), xlim = c(-4, -score),
                geom = "area", fill = "red", alpha = .2) +
  scale_x_continuous(name = "z-value", breaks = seq(-4, 4, std)) +
  labs(title = "99% Confidence Interval", 
       subtitle = "(+/- 2.58 SD)")

```

---

## Confidence interval

- If according to the CLT, 99% of the sampled proportions will fall within +/-
2.58 standard deviations of the population proportion, and

--

- Our best (unbiased, consistent) estimate of the population proportion from
our empirical sample of 100 is .50, and

--

- The standard error of the estimate is therefore .05, then

--

- It follows that 99% of the time, our estimate will be between:
  - Upper bound: $.50 +.05 \times 2.58 = .629$
  - Lower bound: $.50 -.05 \times 2.58 = .371$
  
---

## Practice: 99% confidence intervals

.small[

$p$ | $n$ | SE  | LB | UB 
----|-----|-----|----|----
.2  | 25  |     |    |    
.2  | 100 |     |    |    
.4  | 25  |     |    |    
.4  | 100 |     |    |    
.8  | 25  |     |    |    
.8  | 100 |     |    |    
.6  | 400 |     |    |    
.6  | 900 |     |    |    
.6  | 2500  |     |    |  
.6  | 10000  |     |    | 

]

---

## Using SEs of differences to calculate CIs of differences

$$\widehat{se_{\text{diff}}}=\sqrt{\widehat{se}_1^2 + \widehat{se}_2^2}$$

--

$$CI^.95_{\text{diff}} = (m_1-m_2) \pm 1.96(se_{\text{diff}})$$

---

## Difference in proportions practice

$p_1$ | $p_2$ | $n_1$ | $n_2$ | $p1-p2$ | $se_{\text{diff}}$ | $LB_{CI}$ | $UB_{CI}$
------|-------|-------|-------|---------|-------------------|-----------|-------
.55   | .45   | 100   | 100   |         |                   |           |
.80   | .70   | 400   | 400   |         |                   |           | 

What are the upper and lower bounds of the 95% confidence interval?

???

```
p1-p2	se1	  se2	  sediff LB_diff	UB_diff
0.100	0.050	0.050	0.070	 -0.038	  0.238
0.100	0.020	0.023	0.030	 0.040	  0.160
```

---

## Difference in means practice

$m_1$ | $m_2$ | $s_1$ | $s_2$ | $n_1$  | $n_2$ | $m1-m2$ | $se_{\text{diff}}$ | $LB_{CI}$ | $UB_{CI}$
------|-------|-----|---- |--------|-------|---------|-------------------|-----------|-------
115   | 100   | 15  | 15  | 100    | 100   |         |                   |           |
70    | 64    | 4   | 3   | 400    | 400   |         |                   |           | 

What are the upper and lower bounds of the 95% confidence interval?

???

```
m1-m2	 se1	  se2	  sediff	LB_diff	UB_diff
15.000 1.500	1.500	2.121	  10.842	19.158
6.000	 0.200	0.150	0.250  	5.510	  6.490

```

---

## Important note

- For the binomial distribution, the formula we've been using is a large sample 
approximation.

--

- Consider the inference I'd make if 10% of my sample of 10 people supported
a candidate. What would the 95% confidence interval of the proportion be?

--

- My confidence interval for $p$ would be $[-.086,.286]$. Why is that a problem?

--

- For real small sample problems, you'd use the binomial CDF directly.

---

## The t-distribution

- We use 1.96 (for 95%) or 2.58 (for 99%) based on the normal distribution 
because the CLT allows us to do so regardless of the underlying distribution.

--

- However, this is a large-sample assumption that may not hold in small samples.

--

- The t-statistic is a *heavy-tailed* distribution that gives more probability 
density to "weird" (i.e., extreme) outcomes in small samples.

--

- As the sample gets larger, it gets more and more like the normal distribution.

---

```{r tdist, out.width="95%"}

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + 
  stat_function(fun = dt, args = list(df = 2), color = "red") +
  stat_function(fun = dt, args = list(df = 50), color = "blue") +
  stat_function(fun = dnorm, color = "green") +
  scale_x_continuous(name = "estimate / SE", breaks = seq(-4, 4, 1)) +
  labs(title = "t-distribution", 
       subtitle = "with 2 df (red) and 50 df (blue); normal distribution (green) for reference")

```

---

## Using the t-distribution

.pull-left[

- Values for confidence interval construction aren't fixed at (say) 1.96
or 2.58 like for the normal distribution.

- The t-value depends on the desired interval (i.e., 95%) but also on the 
number of observations (degrees of freedom, or *df*). For example:

]

--

.pull-right[

```{r, echo = TRUE}
qt( p = .975, df = 2 )
qt( p = .975, df = 10 )
qt( p = .975, df = 30 )
qt( p = .975, df = 1000 )

```

]

???

Not going to spend too much time on this – see the book for more. Confidence 
intervals for differences in means use the t.test function in R.

---
class:center,middle,inverse

# Hypothesis testing

---

## The logic of a hypothesis test

1. Calculate what happened in the real world (e.g., the sample difference
in mean between two groups).

--

2. Imagine a world where the *null hypothesis* is true.

--

3. Generate a distribution from the "null world."

--

4. Ask "how often would we see what we saw in the real world if the null world
*was* the real world?"

--

5. If the answer is "pretty much never" (see below), then reject the null world
in favor of the "alternative world."

---

## Proof by contradiction

.left-column[
N v A

N $\rightarrow$ F

~F

~N

A

]

.pull-right[
Either the null or alternative is true.

If null, we should see a particular finding.

We don't see that finding*

N is false (by *modus tollens*).

A must therefore be true.

]

.footnote[ *This isn’t strictly true because ~F is probabilistic, not absolute. 
F represents the range of findings we’d see 95% (or 99%, etc.) of the time if N 
is true. But not 100% of the time! ]

???

This is why rejecting or “failing to reject” is the key point here. Rejecting 
means that we can accept A. If we get F, then we can’t say ~N, which means we 
can’t really say anything…

---

## One-sample hypothesis tests

- Voting is a good example because in a two-candidate election there is a 
reasonable null hypothesis for either candidate: 50%.

--

- Let’s say the null was true – candidate A has 50% 
support in the population. What is the range of estimates we'd expect 99% of 
the time in a sample of 400 people? 900 people?

--

  + The 99% null CI with n = 400 is 43.6% to 56.4% (MoE is 6.4 pp)
  + The 99% null CI with n = 900 is 45.7% to 54.3% (MoE is 4.3 pp)

--

- Only if the proportion estimated is *outside* that range can the null 
hypothesis of equal support be rejected. Otherwise, we “fail to reject” the 
null.

---

## Margin of error

.pull-left[

- Sometimes polls will use "margin of error" to denote estimate precision.

- This refers to the width of the (e.g.) 99% confidence interval where 
$\hat{p} = .5$.

]

--

.pull-right[

```{r, echo=TRUE}
# 99% MoE for N = 900
sqrt(.25/900) * qnorm(.995) 

# 99% MoE for N = 400
sqrt(.25/400) * qnorm(.995)

```


]

---

## Key ideas: test statistic, $p$-value, α-level

- Let's stick with the sample of 900 cases and assume our estimate of the
vote share for candidate A is 55%.

--

- **z-statistic**: how "weird" is our result if the null (50%) is true?

--

  + the difference from the null divided by the SE
  + how many SEs are we away from the null?
  + what is the z-statistic here?
  
--

- **p-value**: converting z-score to what proportion of the time you'd get
a result *at least that extreme* (on either side) by chance if the null is true.

--

- **α-level**: the $p$-value cutoff to reject the null (decided in
advance) or the desired width of the confidence interval.

???

Here z = 3
That corresponds to a p-value of .0027 ( about .003)
If our alpha was .99, then we reject the null

---
.col-text[

```{r, echo = TRUE}
# how different is the real world from the null world?
diff <- .55 - .50

# what is the SE of the estimate under the null?
se <- sqrt( .50 * .50 ) / sqrt( 900 )
se

# how many SEs is what we saw away from the null expectation?
zstat <- diff / se
zstat

# how often would we get a result at least that extreme by chance?
2 * ( 1 - pnorm( zstat ) )

# if this is less than 1-α, then reject the null

```

]

---

## Two-tailed tests

- We multiplied the probability by 2 on the last slide 
(`2*(1 - pnorm( zstat ))`) because we are performing a "two-tailed" test.

--

- We are asking, "what's the probability of getting a result 
*at least that extreme* (i.e., in either direction)?"

--

- One-tailed tests exist and are probably justified in some contexts. But in
most contexts they are used as a form of "p-hacking" and so, by convention, we
don't use them.

---

## Two-sample test of proportions

- Usually we are comparing the *conditional* mean of two groups that *don't* 
add up to 100% (like vote share does).

--

- For example, let's say we estimate that 71.1% of women (N=2764) and 67.4%
of men (N=2219) voted in the last election.

--

- What's the null here?

--

- Which SE do we need?

???

The null is no difference

We need the SE of the difference

---

.col-text[

```{r, echo=TRUE}

diff <- .711 - .674
se1 <- sqrt( .711 * (1-.711) / 2764 )
se2 <- sqrt( .674 * (1-.674) / 2219 ) 
sediff <- sqrt( se1^2 + se2^2 )
zstat <- diff / sediff

2 * ( 1 - pnorm( zstat ) ) # p-value
diff + 2.58 * sediff       # 99% upper bound
diff - 2.58 * sediff       # 99% lower bound

```

]

--

Looks like there is probably at least some difference in the population.

---

## Technical note

Technically this isn’t right. Under the null – that the groups 
have the same probability of voting – the variances would be the same too. 
So we would use the unconditional estimate of $\hat{p}$ (.695) in both the 
equations for `se1` and `se2`.

---

.col-text[

```{r, echo=TRUE}

diff <- .711 - .674
se1 <- sqrt( .695 * (1-.695) / 2764 )
se2 <- sqrt( .695 * (1-.695) / 2219 ) 
sediff <- sqrt( se1^2 + se2^2 )
zstat <- diff / sediff

2 * ( 1 - pnorm( zstat ) ) # p-value
diff + 2.58 * sediff       # 99% upper bound
diff - 2.58 * sediff       # 99% lower bound

```

]

In practice, this didn't matter at all...


---

## Two-sample test for numeric outcomes

- This is very similar to what we just did.

--

- We use the **t-test** to compare the means of two groups because it works
better if the samples are small and converges to the normal in large samples
(thanks to the CLT!).

---

.col-text[

```{r, echo = TRUE}
data(STAR, package = "qss")

# select small and regular classes only
d <- STAR %>% 
  filter(classtype != 3) %>% 
  mutate(smallclass = if_else(classtype == 1, 1L, 0L))

# t-test (don't worry about non-integer df below)
t.test(g4reading ~ smallclass , data = d)

```

]

---

## Summary of hypothesis testing

1. Specify a null and alternative hypothesis.

--

2. Choose a test statistic and level of α to use.

--

3. Create the **reference distribution** (i.e., the sampling distribution
if the null is correct).

--

4. Compute the **p-value** of what you see in your data compared to the 
reference distribution (using two-sided p-value).

--

5. Reject the null iff the p-value is less that or equal to 1-α, otherwise
"fail to reject" it.

---

## Relation between CI and hypothesis test

- If a one-sample CI *excludes* the null, it's the same as rejecting the null 
at the same α-level.

--

- If a one-sample CI *includes* the null value, it's the same as "failing to 
reject the null" at the same α-level.

--

- If the CI of a difference *excludes* 0, it's the same as rejecting the null 
of no difference at the same α-level.

--

- If the CI of a difference *includes* 0, it's the same as "failing to reject 
the null" at the same α-level.

---

## Confidence intervals are better

- They convey "statistical significance" *and* magnitude.

--

- Better to say we have 95% confidence that small classes change reading scores 
somewhere between -2 and 9 points than to say "fail to reject the null 
hypothesis."

--

- Better to say we have 95% confidence that women are between 1/3 and 7 
percentage points more likely to vote than men rather than to "reject the 
null hypothesis that they are equally likely to vote."