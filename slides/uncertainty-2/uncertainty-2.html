<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Uncertainty, Part II</title>
    <meta charset="utf-8" />
    <meta name="author" content=" Stephen Vaisey Duke University" />
    <link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: left, middle, inverse, title-slide

# Uncertainty, Part II
### <small><br>Stephen Vaisey<br>Duke University</small>
### <span style="font-size: 65%;">Last update: 2019-10-29<br><br><i class="fas  fa-link "></i> <a href='stephenvaisey.com'><font color='F3F2F1'>stephenvaisey.com</font></a><br><i class="fab  fa-twitter "></i> <a href='http://twitter.com/vaiseys'><font color='F3F2F1'><span class="citation">@vaiseys</span></font></a><br><i class="fab  fa-github "></i> <a href='https://github.com/vaiseys'><font color='F3F2F1'>vaiseys</font></a></span>

---






class: center, middle

# Ask two questions

---

class: center, middle

# Assessment time

---

## Agenda

1. Confidence intervals (continued)
2. Hypothesis testing
3. Power analysis

---

class:inverse,middle,center

# Confidence intervals

---

## Quantifying uncertainty

- Standard errors are the main ingredient in helping us quantify the 
uncertainty in our estimates.

- Confidence intervals are one of the main things we "cook" with these 
ingredients.

- Confidence intervals allow us to summarize the sampling distribution by
giving plausible upper and lower bounds of our estimate.

---

## CLT and the normal distribution

&lt;img src="uncertainty-2_files/figure-html/normal_crits-1.svg" width="80%" style="display: block; margin: auto;" /&gt;

---

## CLT and the normal distribution

&lt;img src="uncertainty-2_files/figure-html/normal_crit2-1.svg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Confidence interval

- If according to the CLT, 99% of the sampled proportions will fall within +/-
2.58 standard deviations of the population proportion, and

--

- Our best (unbiased, consistent) estimate of the population proportion from
our empirical sample of 100 is .50, and

--

- The standard error of the estimate is therefore .05, then

--

- It follows that 99% of the time, our estimate will be between:
  - Upper bound: `\(.50 +.05 \times 2.58 = .629\)`
  - Lower bound: `\(.50 -.05 \times 2.58 = .371\)`
  
---

## Practice: 99% confidence intervals

.small[

`\(p\)` | `\(n\)` | SE  | LB | UB 
----|-----|-----|----|----
.2  | 25  |     |    |    
.2  | 100 |     |    |    
.4  | 25  |     |    |    
.4  | 100 |     |    |    
.8  | 25  |     |    |    
.8  | 100 |     |    |    
.6  | 400 |     |    |    
.6  | 900 |     |    |    
.6  | 2500  |     |    |  
.6  | 10000  |     |    | 

]

---

## Using SEs of differences to calculate CIs of differences

`$$\widehat{se_{\text{diff}}}=\sqrt{\widehat{se}_1^2 + \widehat{se}_2^2}$$`

--

`$$CI^.95_{\text{diff}} = (m_1-m_2) \pm 1.96(se_{\text{diff}})$$`

---

## Difference in proportions practice

`\(p_1\)` | `\(p_2\)` | `\(n_1\)` | `\(n_2\)` | `\(p1-p2\)` | `\(se_{\text{diff}}\)` | `\(LB_{CI}\)` | `\(UB_{CI}\)`
------|-------|-------|-------|---------|-------------------|-----------|-------
.55   | .45   | 100   | 100   |         |                   |           |
.80   | .70   | 400   | 400   |         |                   |           | 

What are the upper and lower bounds of the 95% confidence interval?

???

```
p1-p2	se1	  se2	  sediff LB_diff	UB_diff
0.100	0.050	0.050	0.070	 -0.038	  0.238
0.100	0.020	0.023	0.030	 0.040	  0.160
```

---

## Difference in means practice

`\(m_1\)` | `\(m_2\)` | `\(s_1\)` | `\(s_2\)` | `\(n_1\)`  | `\(n_2\)` | `\(m1-m2\)` | `\(se_{\text{diff}}\)` | `\(LB_{CI}\)` | `\(UB_{CI}\)`
------|-------|-----|---- |--------|-------|---------|-------------------|-----------|-------
115   | 100   | 15  | 15  | 100    | 100   |         |                   |           |
70    | 64    | 4   | 3   | 400    | 400   |         |                   |           | 

What are the upper and lower bounds of the 95% confidence interval?

???

```
m1-m2	 se1	  se2	  sediff	LB_diff	UB_diff
15.000 1.500	1.500	2.121	  10.842	19.158
6.000	 0.200	0.150	0.250  	5.510	  6.490

```

---

## Important note

- For the binomial distribution, the formula we've been using is a large sample 
approximation.

--

- Consider the inference I'd make if 10% of my sample of 10 people supported
a candidate. What would the 95% confidence interval of the proportion be?

--

- My confidence interval for `\(p\)` would be `\([-.086,.286]\)`. Why is that a problem?

--

- For real small sample problems, you'd use the binomial CDF directly.

---

## The t-distribution

- We use 1.96 (for 95%) or 2.58 (for 99%) based on the normal distribution 
because the CLT allows us to do so regardless of the underlying distribution.

--

- However, this is a large-sample assumption that may not hold in small samples.

--

- The t-statistic is a *heavy-tailed* distribution that gives more probability 
density to "weird" (i.e., extreme) outcomes in small samples.

--

- As the sample gets larger, it gets more and more like the normal distribution.

---

&lt;img src="uncertainty-2_files/figure-html/tdist-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

## Using the t-distribution

.pull-left[

- Values for confidence interval construction aren't fixed at (say) 1.96
or 2.58 like for the normal distribution.

- The t-value depends on the desired interval (i.e., 95%) but also on the 
number of observations (degrees of freedom, or *df*). For example:

]

--

.pull-right[


```r
qt( p = .975, df = 2 )
```

```
## [1] 4.302653
```

```r
qt( p = .975, df = 10 )
```

```
## [1] 2.228139
```

```r
qt( p = .975, df = 30 )
```

```
## [1] 2.042272
```

```r
qt( p = .975, df = 1000 )
```

```
## [1] 1.962339
```

]

???

Not going to spend too much time on this – see the book for more. Confidence 
intervals for differences in means use the t.test function in R.

---
class:center,middle,inverse

# Hypothesis testing

---

## The logic of a hypothesis test

1. Calculate what happened in the real world (e.g., the sample difference
in mean between two groups).

--

2. Imagine a world where the *null hypothesis* is true.

--

3. Generate a distribution from the "null world."

--

4. Ask "how often would we see what we saw in the real world if the null world
*was* the real world?"

--

5. If the answer is "pretty much never" (see below), then reject the null world
in favor of the "alternative world."

---

## Proof by contradiction

.left-column[
N v A

N `\(\rightarrow\)` F

~F

~N

A

]

.right-column[
Either the null or alternative is true.

If null, we should see a particular finding.

We don't see that finding*

N is false (by *modus tollens*).

A must therefore be true.

]

.footnote[ *This isn’t strictly true because ~F is probabilistic, not absolute. 
F represents the range of findings we’d see 95% (or 99%, etc.) of the time if N 
is true. But not 100% of the time! ]

???

This is why rejecting or “failing to reject” is the key point here. Rejecting 
means that we can accept A. If we get F, then we can’t say ~N, which means we 
can’t really say anything…

---

## One-sample hypothesis tests

- Voting is a good example because in a two-candidate election there is a 
reasonable null hypothesis for either candidate: 50%.

--

- Let’s say the null was true – candidate A has 50% 
support in the population. What is the range of estimates we'd expect 99% of 
the time in a sample of 400 people? 900 people?

--

  + The 99% null CI with n = 400 is 43.6% to 56.4% (MoE is 6.4 pp)
  + The 99% null CI with n = 900 is 45.7% to 54.3% (MoE is 4.3 pp)

--

- Only if the proportion estimated is *outside* that range can the null 
hypothesis of equal support be rejected. Otherwise, we “fail to reject” the 
null.

---

## Margin of error

.pull-left[

- Sometimes polls will use "margin of error" to denote estimate precision.

- This refers to the width of the (e.g.) 99% confidence interval where 
`\(\hat{p} = .5\)`.

]

--

.pull-right[


```r
# 99% MoE for N = 900
sqrt(.25/900) * qnorm(.995) 
```

```
## [1] 0.04293049
```

```r
# 99% MoE for N = 400
sqrt(.25/400) * qnorm(.995)
```

```
## [1] 0.06439573
```


]

---

## Key ideas: test statistic, `\(p\)`-value, α-level

- Let's stick with the sample of 900 cases and assume our estimate of the
vote share for candidate A is 55%.

--

- **z-statistic**: how "weird" is our result if the null (50%) is true?

--

  + the difference from the null divided by the SE
  + how many SEs are we away from the null?
  + what is the z-statistic here?
  
--

- **p-value**: converting z-score to what proportion of the time you'd get
a result *at least that extreme* (on either side) by chance if the null is true.

--

- **α-level**: the `\(p\)`-value cutoff to reject the null (decided in
advance) or the desired width of the confidence interval.

???

Here z = 3
That corresponds to a p-value of .0027 ( about .003)
If our alpha was .99, then we reject the null

---
.col-text[


```r
# how different is the real world from the null world?
diff &lt;- .55 - .50

# what is the SE of the estimate?
se &lt;- sqrt( .55 * .45 ) / sqrt( 900 )
se
```

```
## [1] 0.01658312
```

```r
# how many SEs is what we saw away from the null expectation?
zstat &lt;- diff / se
zstat
```

```
## [1] 3.015113
```

```r
# how often would we get a result at least that extreme by chance?
2 * ( 1 - pnorm( zstat ) )
```

```
## [1] 0.002568832
```

```r
# if this is less than 1-α, then reject the null
```

]

---

## Two-tailed tests

- We multiplied the probability by 2 on the last slide 
(`2*(1 - pnorm( zstat ))`) because we are performing a "two-tailed" test.

--

- We are asking, "what's the probability of getting a result 
*at least that extreme* (i.e., in either direction)?"

--

- One-tailed tests exist and are probably justified in some contexts. But in
most contexts they are used as a form of "p-hacking" and so, by convention, we
don't use them.

---

## Two-sample test of proportions

- Usually we are comparing the *conditional* mean of two groups that *don't* 
add up to 100% (like vote share does).

--

- For example, let's say we estimate that 71.1% of women (N=2764) and 67.4%
of men (N=2219) voted in the last election.

--

- What's the null here?

--

- Which SE do we need?

???

The null is no difference

We need the SE of the difference

---

.col-text[


```r
diff &lt;- .711 - .674
se1 &lt;- sqrt( .711 * (1-.711) / 2764 )
se2 &lt;- sqrt( .674 * (1-.674) / 2219 ) 
sediff &lt;- sqrt( se1^2 + se2^2 )
zstat &lt;- diff / sediff

2 * ( 1 - pnorm( zstat ) ) # p-value
```

```
## [1] 0.004952132
```

```r
diff + 2.58 * sediff       # 99% upper bound
```

```
## [1] 0.07096995
```

```r
diff - 2.58 * sediff       # 99% lower bound
```

```
## [1] 0.003030055
```

]

--

Looks like there is probably at least some difference in the population.

---

## Technical note

Technically this isn’t right. Under the null – that the groups 
have the same probability of voting – the variances would be the same too. 
So we would use the unconditional estimate of `\(\hat{p}\)` (.695) in both the 
equations for `se1` and `se2`.

---

.col-text[


```r
diff &lt;- .711 - .674
se1 &lt;- sqrt( .695 * (1-.695) / 2764 )
se2 &lt;- sqrt( .695 * (1-.695) / 2219 ) 
sediff &lt;- sqrt( se1^2 + se2^2 )
zstat &lt;- diff / sediff

2 * ( 1 - pnorm( zstat ) ) # p-value
```

```
## [1] 0.004810872
```

```r
diff + 2.58 * sediff       # 99% upper bound
```

```
## [1] 0.07085788
```

```r
diff - 2.58 * sediff       # 99% lower bound
```

```
## [1] 0.003142117
```

]

In practice, this didn't matter at all...


---

## Two-sample test for numeric outcomes

- This is very similar to what we just did.

--

- We use the **t-test** to compare the means of two groups because it works
better if the samples are small and converges to the normal in large samples
(thanks to the CLT!).

---

.col-text[


```r
data(STAR, package = "qss")

# select small and regular classes only
d &lt;- STAR %&gt;% 
  filter(classtype != 3) %&gt;% 
  mutate(smallclass = if_else(classtype == 1, 1L, 0L))

# t-test (don't worry about non-integer df below)
t.test(g4reading ~ smallclass , data = d)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  g4reading by smallclass
## t = -1.3195, df = 1541.2, p-value = 0.1872
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8.706055  1.703591
## sample estimates:
## mean in group 0 mean in group 1 
##        719.8900        723.3912
```

]

---

## Summary of hypothesis testing

1. Specify a null and alternative hypothesis.

--

2. Choose a test statistic and level of α to use.

--

3. Create the **reference distribution** (i.e., the sampling distribution
if the null is correct).

--

4. Compute the **p-value** of what you see in your data compared to the 
reference distribution (using two-sided p-value).

--

5. Reject the null iff the p-value is less that or equal to 1-α, otherwise
"fail to reject" it.

---

## Relation between CI and hypothesis test

- If a one-sample CI *excludes* the null, it's the same as rejecting the null 
at the same α-level.

--

- If a one-sample CI *includes* the null value, it's the same as "failing to 
reject the null" at the same α-level.

--

- If the CI of a difference *excludes* 0, it's the same as rejecting the null 
of no difference at the same α-level.

--

- If the CI of a difference *includes* 0, it's the same as "failing to reject 
the null" at the same α-level.

---

## Confidence intervals are better

- They convey "statistical significance" *and* magnitude.

--

- Better to say we have 95% confidence that small classes change reading scores 
somewhere between -2 and 9 points than to say "fail to reject the null 
hypothesis."

--

- Better to say we have 95% confidence that women are between 1/3 and 7 
percentage points more likely to vote than men rather than to "reject the 
null hypothesis that they are equally likely to vote."
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
