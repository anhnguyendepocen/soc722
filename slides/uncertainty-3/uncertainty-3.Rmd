---
title: "Uncertainty, Part III"
author: "<small><br>Stephen Vaisey<br>Duke University</small>"
date: "<span style = 'font-size: 65%;'>Last update: `r Sys.Date()`<br><br>`r icon::fa_link()` <a href='stephenvaisey.com'><font color='F3F2F1'>stephenvaisey.com</font></a><br>`r icon::fa_twitter()` <a href='http://twitter.com/vaiseys'><font color='F3F2F1'>@vaiseys</font></a><br>`r icon::fa_github()` <a href='https://github.com/vaiseys'><font color='F3F2F1'>vaiseys</font></a></span> "
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    self_contained: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["left", "middle", "inverse"]
---

```{r setup, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  fig.asp = 0.618,
  fig.retina = 3,
  fig.width = 6,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  out.width = "80%"
)
options(knitr.table.format = "html")
options(knitr.kable.NA = '   ')

library(here)
library(knitr)
library(tidyverse)
library(broom)
library(DiagrammeR)
theme_set(theme_minimal(base_family = "Lato"))

```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#00539B" ,
  white_color = "#FFFFFF" , # was F3F2F1
  black_color = "#262626" ,
  header_h1_font_size = "110px",
  header_h2_font_size = "60px",
  header_h3_font_size = "45px",
  text_font_size = "28px" ,
  code_font_size = ".65em" ,
  header_color = "#C84E00" ,
  text_slide_number_font_size = "0.6em" ,
  code_highlight_color = "rgba(161,183,13,0.5)" ,
  link_color = "#993399" ,
  text_font_google = google_font("Lato", "400", "400i", "700") ,
  code_font_google = google_font("Roboto Mono", "400") ,
  extra_css = list(
    ".remark-slide-content" = list("padding-top" = "60px") ,
    "h1" = list("margin" = "0 0 20px 0") ,
    "h2" = list("margin" = "0 0 20px 0") ,
    "h3" = list("margin" = "0 0 20px 0") ,
    "th, td" = list( "padding" = "10px" ),
    ".small" = list("font-size" = "60%"),
    ".small-code" = list("font-size" = ".6em") ,
    ".smaller-code" = list("font-size" = ".5em") ,
    ".red" = list("color" = "red") ,
    ".blue" = list("color" = "blue") ,
    ".col-text" = list("font-size" = "80%"))
)


```

class: center, middle

# Ask two questions

---

class: center, middle

# Assessment time


---

## Agenda

1. Power

2. Uncertainty in the linear regression model

---

class: center, middle, inverse

# Power

---

## Power

- **Power** is the probability of rejecting the null hypothesis (i.e., "finding 
an effect") if the true effect is a certain hypothetical magnitude.

--

- Power increases with two main factors:
  + the hypothetical effect size gets bigger (easier to detect)
  + the sample size increases (more data to detect the difference)

---

## Sample sizes, effect sizes, and power

```{r powercurve1}
# power curve: t-test
n <- seq(1:150)*10
delta <- c(.1, .2, .3, .4, .5)

x <- expand.grid(n = n,
                 delta = delta)

myPower <- function(n, delta) {
  power.t.test(n = n,
               delta = delta,
               sig.level = .05, # change to .005 for other example
               type = "two.sample",
               alternative = "two.sided") -> temp
  return(temp$power)
}

d <- 
  x %>% 
  mutate(power = myPower(n = n, delta = delta),
         delta = factor(delta))

ggplot(d, aes(x = n, y = power, color = delta)) +
  geom_line(size = 1) +
  geom_hline(yintercept = .8, linetype = "dotted") +
  labs(y = expression(Power~"for"~alpha~"=.05"), 
       x = "Sample Size of Each Group")

```

---

## Types I and II error and Bayes' theorem

- If we use conventional levels of α (.05) and power (.80), what is the 
probability of a true positive? A false positive?
--

- So what is the likelihood ratio of the "test" for a real effect?
--

- According to Bayes' theorem, what other piece of information do we need to 
know to calculate $P$(real effect | significant result)?
--

- What if our prior probability that an effect is real is 20%? 10%? 5%? 1%? 
What are the corresponding posterior probabilities?
--

- What does this mean about "counterintuitive" findings? What if we changed 
the "default" p-value to .005?


???

P true positive = .80
P false positive = .05
LR = .80/.05 = 16
We need the prior odds (or prior probability)

PriProb	PriOdds	LR	PostOdds	PostProb
0.200	0.250	16	4.000	0.800
0.100	0.111	16	1.778	0.640
0.050	0.053	16	0.842	0.457
0.010	0.010	16	0.162	0.139

PriProb	PriOdds	LR	PostOdds	PostProb
0.200	0.250	160	40.000	0.976
0.100	0.111	160	17.778	0.947
0.050	0.053	160	8.421	0.894
0.010	0.010	160	1.616	0.618


---

## Power with α = .005

```{r powercurve2}
myPower <- function(n, delta) {
  power.t.test(n = n,
               delta = delta,
               sig.level = .005, # change to .005 for other example
               type = "two.sample",
               alternative = "two.sided") -> temp
  return(temp$power)
}

d <- 
  x %>% 
  mutate(power = myPower(n = n, delta = delta),
         delta = factor(delta))

ggplot(d, aes(x = n, y = power, color = delta)) +
  geom_line(size = 1) +
  geom_hline(yintercept = .8, linetype = "dotted") +
  labs(y = expression(Power~"for"~alpha~"=.005"), 
       x = "Sample Size of Each Group")


```

---

## Calculating sample size for an experiment

.col-text[

.pull-left[
```{r, echo = TRUE}
power.t.test(power = .8,
             sig.level = .05,
             delta = .3,
             type = "two.sample",
             alternative = "two.sided")


```


]

--

.pull-right[
```{r, echo = TRUE}
power.t.test(power = .8,
             sig.level = .005,
             delta = .3,
             type = "two.sample",
             alternative = "two.sided")

```


]

]

--

.footnote[This would cost 70% more to field but reduce false positives through 
p-hacking (e.g., subsetting or otherwise tweaking to find "significance")
]

---

## How large a difference can we detect in the GSS?

```{r, echo = TRUE}
pwr::pwr.t2n.test(sig.level = .005,
                  power = .8,
                  n1 = 490,  # 2016 Blacks
                  n2 = 2100, # 2016 Whites 
                  alternative = "two.sided")
```

---

class:center,middle,inverse

# Uncertainty in the LRM

---

## Linear regression model (LRM)

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

This is a *conditional* mean operator that can also be written:

$$\mathbb{E}(Y | X=x) = \beta_0 + \beta_1 x $$

---

## Random numbers are samples from a population

.pull-left[

```{r, echo=TRUE, eval=FALSE}
set.seed(125)
d <- tibble(x = rnorm(100),
            e = rnorm(100))

d <- d %>% 
  mutate(y = .4*x + e)

ggplot(d, aes(y = y, x = x)) +
  geom_smooth(method = "lm",
              se = FALSE) +
  geom_point()

```

]

.pull-right[

```{r lrm1, out.width="95%"}
set.seed(125)
d <- tibble(x = rnorm(100),
            e = rnorm(100))

d <- d %>% 
  mutate(y = .4*x + e)

ggplot(d, aes(y = y, x = x)) +
  geom_smooth(method = "lm",
              se = FALSE) +
  geom_point()


```


]

---

## LRM as a data-generating process (DGP)

- Sometimes we don't just want to draw lines through a cloud
- We think there is a true function in the world that produced the data and we 
want to know what it is, like:
  + the effect of education on income
  + the effect of gender on callback rate
  + the effect of GDP on life expectancy
  + the effect of friends' attitudes on a person's attitude
  + etc.
  
---

## Simulating LRM sampling distribution

```{r, echo = TRUE}

## get a dataset of datasets
n1 <- 900 # individual dataset size
ns <- 1000 # number of datasets

d2 <- tibble(sim = rep(1:n1, ns))
d2 <- d2 %>% 
  arrange(sim) %>% 
  mutate(x = rnorm(n1*ns),
         e = rnorm(n1*ns),
         y = .4*x + e) %>% 
  group_by(sim) %>% 
  nest()

```

--

We're creating a world where $y_i=.4x_i+\epsilon_i$ and taking 1000 samples of
900 "respondents" from it.

---

## Working with the data

```{r, echo = TRUE}
# get useable OLS function for map
myOLS <- function(df) { 
  lm(y ~ x, data = df)
}

# create dataset of regression results and get ready to plot
simResults <- d2 %>% 
  mutate(model = map(data, myOLS),
         tidied = map(model, broom::tidy)) %>%
  unnest(tidied, .drop = TRUE) %>%
  filter(term != "(Intercept)") %>% 
  select(sim, estimate) %>% 
  rename(beta = estimate)

```

---

## Plotting the results

```{r}
ggplot(simResults, aes(x = beta)) +
  geom_histogram(binwidth=.005, fill = "gray", color = "white") +
  geom_vline(xintercept = .4, linetype = "dashed") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Simulated Sampling Distribution of beta", 
       subtitle = "1000 datasets of 900 cases each",
       x = "estimate of beta")

```

---

## Theory-based SE of $\hat{\beta}}

$$\text{SE of } \hat{\beta} = \frac{\frac{1}{n}\sum\hat{\epsilon}_i^2}{\sum(X_i-\bar{X})^2}$$

Don't worry too much about this. Just know the basic idea of what it means.

---

## Inference about regression coefficients

With the **estimate** and the **standard error**, we can do all the same things 
we did with means, proportions, and differences.

.col-text[

.pull-left[

```{r, echo = TRUE}
lm(y ~ x, data = d) %>% 
  tidy() %>%
  filter(term == "x") %>% 
  select(estimate, std.error)

```

]

.pull-right[

- What is the 95% confidence interval on $\hat{\beta}$?
- What is the 99% confidence interval on $\hat{\beta}$?
- What is the null hypothesis for $\beta$?
- What is the alternative hypothesis?
- What is the z- (or t-) statistic of the test?
- What is the approximate p-value?

]
]

???

95% CI [.23, .59]
99% CI [.17, .64]
Null is beta = 0
Alternative is != 0
Test stat is 4.5
Approximate p-value is very low -- .0000185

---

## Assumptions needed for inference

- Two main assumptions covered in this chapter are exogeneity and 
homoscedasticity:

  - **Exogeneity**: no correlation between $X$ and $\epsilon$
  - **Homoscedasticity**: the variance of $\epsilon$ doesn’t depend on $X$
--

- Dealing with heteroskedasticity is pretty easy (not covered here)

- Dealing with violations of exogeneity is hard

---

## Controlling for confounders

- We try to address concerns about endogeneity by controlling for other 
confounding causes of Y

- This is very much like subsetting the data by a third variable (Z) and 
estimating the regression within the subgroups

---

## Check out visualizations for intuition

.pull-left[

![](http://nickchk.com/anim/Animation%20of%20Control.gif)
]

.pull-right[See nickchk.com for graphs like these!]